{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eoDGJaE3od8I",
        "ROsIs-wyoV7u",
        "Se2OsM1BonAQ",
        "PG5SY_iHpm4V",
        "65rTvJJpscn8",
        "SIc81i_4RZov",
        "OkaFqdUESYT3",
        "X0z4NBYxVRUA",
        "dPpAJzf8ynAK",
        "eQefhFFVzWbC",
        "jPE4RavQj2CF",
        "X1gqT3-XpnQ8",
        "geY-v9N_r6TR",
        "FkdslTQTs6Ad",
        "6g9yzWE73bG3",
        "CxPolCeF0rU-",
        "M7pBL7Em6Y9Y",
        "RF8oZpoV8gNs",
        "nTbY9I3sAAfy",
        "ZXexvrkXACZ7",
        "KhRqh7EsAKoU",
        "Wdv8gXGQA2O0",
        "4Usp8QUFBeLM",
        "UN7V_aS2BgKw",
        "8xrQz5LRBhTb",
        "zEsBRcqNApG1",
        "vBhkjGamiAyq",
        "CO-YnAXWB-3S",
        "Y0g4adwkcPjc",
        "QqHT07Q4bMM_",
        "nR3iJbTUdc2U",
        "YMFqleBhnS01",
        "HygmfYAKnX_6",
        "78I7gnWKbJhx",
        "t2qxz9lsgDk6",
        "OhwLaj4zX1J6",
        "vSVAPQZlmcm5",
        "RSknFN-pZmwc",
        "7ZrvUIeXZ0F4",
        "XYqn3MvvtiWk",
        "bftSxYhe0Eq9",
        "PlcDLfHB0G9a",
        "FB1GOmab075X",
        "B0rRTFq8cfTG",
        "mhGRfNdskWVb",
        "99SUs6T1jqmX",
        "AcZHMzpCe-kY",
        "oymPARJDouhT",
        "4nPDcbx6qCIQ",
        "tN7ti-qO6ay3",
        "fu70Aq120Vl2",
        "Zay2ROEG0XLr",
        "-HbAAy1z0ZAH",
        "wk6LAIzS6b8S",
        "9fmEAiO13OwI",
        "SiYHqiCH3SKN",
        "kfBhaNjT8X02",
        "XUI0O9BR8hxx",
        "ZYsEXgBm8jOV",
        "diZ3VNMmhp-K",
        "6hpVUSjwovOZ",
        "B0DwMB-PvuJs",
        "C9WDBtzh7HV5",
        "NjQaoaaBz6s2",
        "y_94pXPxOBL_",
        "kjhOtVF1FLTi",
        "JKCQZcqJHJ6S",
        "q03BdKzvt_kv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadiaHolmlund/BDS_M1_Exam/blob/main/BDS_M1_Exam_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries for examples"
      ],
      "metadata": {
        "id": "eoDGJaE3od8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification"
      ],
      "metadata": {
        "id": "2Cnnx9uhodPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import datasets for examples"
      ],
      "metadata": {
        "id": "ROsIs-wyoV7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_courses = pd.read_csv('https://raw.githubusercontent.com/HamidBekamiri/Data_Science_Handbook/main/udemy_courses_info.csv')\n",
        "df_courses_year= pd.read_csv('https://raw.githubusercontent.com/HamidBekamiri/Data_Science_Handbook/main/udemy_courses_year.csv')\n",
        "df_courses_2017 = pd.read_csv('https://raw.githubusercontent.com/HamidBekamiri/Data_Science_Handbook/main/df_courses_2017.csv')"
      ],
      "metadata": {
        "id": "HvG6db3Wkhro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_courses.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "qVdFIoZ8oxZr",
        "outputId": "21e20991-f87c-4064-9445-6a54948c996a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   course_id                                       course_title  \\\n",
              "0    1006314  Financial Modeling for Business Analysts and C...   \n",
              "\n",
              "                                                 url  is_paid  price  \\\n",
              "0  https://www.udemy.com/financial-modeling-for-b...     True     45   \n",
              "\n",
              "   num_subscribers  num_reviews  num_lectures               level  \\\n",
              "0             2174         74.0          51.0  Intermediate Level   \n",
              "\n",
              "   content_duration           subject  \n",
              "0               2.5  Business Finance  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-46028627-d503-4ec3-8578-d2a0d494878a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>course_id</th>\n",
              "      <th>course_title</th>\n",
              "      <th>url</th>\n",
              "      <th>is_paid</th>\n",
              "      <th>price</th>\n",
              "      <th>num_subscribers</th>\n",
              "      <th>num_reviews</th>\n",
              "      <th>num_lectures</th>\n",
              "      <th>level</th>\n",
              "      <th>content_duration</th>\n",
              "      <th>subject</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1006314</td>\n",
              "      <td>Financial Modeling for Business Analysts and C...</td>\n",
              "      <td>https://www.udemy.com/financial-modeling-for-b...</td>\n",
              "      <td>True</td>\n",
              "      <td>45</td>\n",
              "      <td>2174</td>\n",
              "      <td>74.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>Intermediate Level</td>\n",
              "      <td>2.5</td>\n",
              "      <td>Business Finance</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46028627-d503-4ec3-8578-d2a0d494878a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-46028627-d503-4ec3-8578-d2a0d494878a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-46028627-d503-4ec3-8578-d2a0d494878a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_courses_year.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "a5wq9YORszyQ",
        "outputId": "51cc77a1-dcbd-41f8-9886-fbbc0c28ad15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   course_id  year\n",
              "0    1006314  2016"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-312a08c3-197e-43ae-9c66-99332b648f26\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>course_id</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1006314</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-312a08c3-197e-43ae-9c66-99332b648f26')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-312a08c3-197e-43ae-9c66-99332b648f26 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-312a08c3-197e-43ae-9c66-99332b648f26');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_courses_2017.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "oH5rWZPUt4Gy",
        "outputId": "da725ea8-87e2-460d-fd70-7d95ae5f6a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   course_id                        course_title  \\\n",
              "0    1070968  Ultimate Investment Banking Course   \n",
              "\n",
              "                                                 url  is_paid  price  \\\n",
              "0  https://www.udemy.com/ultimate-investment-bank...     True    200   \n",
              "\n",
              "   num_subscribers  num_reviews  num_lectures       level  content_duration  \\\n",
              "0             2147         23.0          51.0  All Levels               1.5   \n",
              "\n",
              "            subject  year  revenue  \n",
              "0  Business Finance  2017   429400  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f42fc1f-2d0c-484e-aec4-ab01515efdb2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>course_id</th>\n",
              "      <th>course_title</th>\n",
              "      <th>url</th>\n",
              "      <th>is_paid</th>\n",
              "      <th>price</th>\n",
              "      <th>num_subscribers</th>\n",
              "      <th>num_reviews</th>\n",
              "      <th>num_lectures</th>\n",
              "      <th>level</th>\n",
              "      <th>content_duration</th>\n",
              "      <th>subject</th>\n",
              "      <th>year</th>\n",
              "      <th>revenue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1070968</td>\n",
              "      <td>Ultimate Investment Banking Course</td>\n",
              "      <td>https://www.udemy.com/ultimate-investment-bank...</td>\n",
              "      <td>True</td>\n",
              "      <td>200</td>\n",
              "      <td>2147</td>\n",
              "      <td>23.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>All Levels</td>\n",
              "      <td>1.5</td>\n",
              "      <td>Business Finance</td>\n",
              "      <td>2017</td>\n",
              "      <td>429400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f42fc1f-2d0c-484e-aec4-ab01515efdb2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f42fc1f-2d0c-484e-aec4-ab01515efdb2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f42fc1f-2d0c-484e-aec4-ab01515efdb2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_courses.shape)\n",
        "print(df_courses_year.shape)\n",
        "print(df_courses_2017.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5C44hUqs7SS",
        "outputId": "6e9c86ef-cdd9-4c96-d34e-aae762f04d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2959, 11)\n",
            "(2959, 2)\n",
            "(713, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is the best way to print the number of values in a column?"
      ],
      "metadata": {
        "id": "Se2OsM1BonAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_courses['course_id'].value_counts().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJFFqvO6kd2l",
        "outputId": "040bfe3f-eb5f-4796-e05e-b161aef69bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2959"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative but not as neat\n",
        "df_courses['course_id'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYDsVz_OqeqP",
        "outputId": "7daf23f5-f54a-4839-e763-1ce83b84c382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2959,)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.\tWhat is the best way to print the highest and lowest value of a column?"
      ],
      "metadata": {
        "id": "PG5SY_iHpm4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_courses['price'].max())\n",
        "print(df_courses['price'].min())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rY9jPtgly4d",
        "outputId": "6e62d4a7-a3b6-45ab-da9e-3c00c17c96b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative but not as neat, includes also count, mean, percentiles etc.\n",
        "print(df_courses['price'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgXvdG-sqa-Y",
        "outputId": "5dba3c4f-8b8f-4849-8a39-82e9ffadf25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    2959.000000\n",
            "mean       63.773234\n",
            "std        59.121061\n",
            "min         0.000000\n",
            "25%        20.000000\n",
            "50%        45.000000\n",
            "75%        95.000000\n",
            "max       200.000000\n",
            "Name: price, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.\tHow do you combine two data sets?"
      ],
      "metadata": {
        "id": "65rTvJJpscn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- .merge(): Combining data on common columns or indices.\n",
        "- .concat(): Combining DataFrames across rows or columns.\n",
        "- .join(): Combining data on a key column or an index."
      ],
      "metadata": {
        "id": "gv0HeTDkPcXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .merge()\n",
        "The datasets are merged based on column 'course_id' or indices, i.e. the number of rows remain while the number of columns increase\n",
        "\n",
        "(the column 'year' is added, since this is the only column that differs between the datasets)\n",
        "\n",
        "Note: () brackets"
      ],
      "metadata": {
        "id": "SIc81i_4RZov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_courses_merge = pd.merge(df_courses, df_courses_year)\n",
        "df_courses_merge.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGFsmIkUmgOl",
        "outputId": "4f93d82d-2000-42f9-849a-1e102f942f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2959, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .concat()\n",
        "\n",
        "The datasets are concatenated based on columns and rows, i.e. the number of rows increase and the number of columns increase\n",
        "\n",
        "(the column 'revenue' is added, since this is the only column that differs between the datasets. The column has value NaN in the rows from df_courses_merge while the rows from df_courses_2017 is added below)\n",
        "\n",
        "Note: ([]) brackets"
      ],
      "metadata": {
        "id": "OkaFqdUESYT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_courses_concat = pd.concat([df_courses_merge, df_courses_2017])\n",
        "df_courses_concat.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAYlJl-Yw4IW",
        "outputId": "ede36f0f-f18f-433f-c61d-616ba727de5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3672, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .join()\n",
        "- Inner Join: returns a dataframe with only those rows that have common characteristics.\n",
        "- Full Join: returns all those records which either have a match in the left or right dataframe (missing values will be NaN on both left and right). Also called outer join.\n",
        "- Left Join: returns a dataframe containing all the rows of the left dataframe ((missing values will be NaN on the right)\n",
        "- Right Join: returns a dataframe containing all the rows of the right dataframe (missing values will be NaN on the left)\n",
        "\n",
        "Note: () brackets\n",
        "\n",
        "Note: Cannot run example on current datasets\n",
        "\n",
        "Syntax: DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)"
      ],
      "metadata": {
        "id": "X0z4NBYxVRUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.\tWhat is the purpose of groupby operations?"
      ],
      "metadata": {
        "id": "dPpAJzf8ynAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Groupby operations are used to group large amounts of data and compute operations on these groups. Groupby is a 3-step process:\n",
        "\n",
        "Splitting : It is a process in which we split data into groups by applying some conditions on datasets.\n",
        "\n",
        "Applying : It is a process in which we apply a function to each group independently (e.g. count, sum, agg)\n",
        "\n",
        "Combining : It is a process in which we combine different datasets after applying groupby and results into a data structure\n",
        "\n",
        "Note: ([]] brackets + [] brackets"
      ],
      "metadata": {
        "id": "b__r7FMof29j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the number of courses within each subject?\n",
        "df_courses_concat.groupby(['subject'])['course_id'].count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7L_3GevclRg",
        "outputId": "3fb6fb25-e3eb-44b4-9592-c873759b9325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "subject\n",
              "Business Finance       1191\n",
              "Graphic Design          602\n",
              "Musical Instruments     680\n",
              "Web Development        1199\n",
              "Name: course_id, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the number of courses in each level of course within each subject?\n",
        "df_courses_concat.groupby(['subject','level'])['course_id'].count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv6Ia854mgK_",
        "outputId": "7b7fe086-66ed-4187-e877-b0da1913fab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "subject              level             \n",
              "Business Finance     All Levels            693\n",
              "                     Beginner Level        339\n",
              "                     Expert Level           31\n",
              "                     Intermediate Level    128\n",
              "Graphic Design       All Levels            298\n",
              "                     Beginner Level        242\n",
              "                     Expert Level            5\n",
              "                     Intermediate Level     57\n",
              "Musical Instruments  All Levels            276\n",
              "                     Beginner Level        296\n",
              "                     Expert Level            7\n",
              "                     Intermediate Level    101\n",
              "Web Development      All Levels            658\n",
              "                     Beginner Level        391\n",
              "                     Expert Level           15\n",
              "                     Intermediate Level    135\n",
              "Name: course_id, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the number of subscribers in each level of course within each subject?\n",
        "df_courses_concat.groupby(['subject','level'])['num_subscribers'].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5MYPPbHyzNr",
        "outputId": "d8a4c9b8-a1a4-4fb0-ecc5-da1731023cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "subject              level             \n",
              "Business Finance     All Levels            1047208\n",
              "                     Beginner Level         647007\n",
              "                     Expert Level            30146\n",
              "                     Intermediate Level     144350\n",
              "Graphic Design       All Levels             688332\n",
              "                     Beginner Level         315757\n",
              "                     Expert Level             1008\n",
              "                     Intermediate Level      58051\n",
              "Musical Instruments  All Levels             328170\n",
              "                     Beginner Level         429030\n",
              "                     Expert Level             5431\n",
              "                     Intermediate Level      84058\n",
              "Web Development      All Levels            4808081\n",
              "                     Beginner Level        2660049\n",
              "                     Expert Level            13611\n",
              "                     Intermediate Level     455546\n",
              "Name: num_subscribers, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the number of courses which have been provided each year?\n",
        "df_courses_concat.groupby(['year'])['course_id'].count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew-b4-ahy_QL",
        "outputId": "aa129a06-e822-4792-e3be-1fcb74a0526a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "year\n",
              "2011       5\n",
              "2012      45\n",
              "2013     201\n",
              "2014     490\n",
              "2015    1014\n",
              "2016    1204\n",
              "2017     713\n",
              "Name: course_id, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the number of free courses in each subject?\n",
        "# Every subject category has free or paid courses, that are segregated by course id.\n",
        "df_courses_concat.groupby(['subject','is_paid'])['course_id'].count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXib1mYVzBNh",
        "outputId": "72c3ec2d-da43-475f-a173-e580630f1185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "subject              is_paid\n",
              "Business Finance     False        96\n",
              "                     True       1095\n",
              "Graphic Design       False        35\n",
              "                     True        567\n",
              "Musical Instruments  False        46\n",
              "                     True        634\n",
              "Web Development      False       133\n",
              "                     True       1066\n",
              "Name: course_id, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the number of courses and the number of subscribers in each level of course within each subject?\n",
        "df_courses_concat.groupby(['subject','level'])['course_id','num_subscribers'].agg({'course_id':'count', 'num_subscribers':'sum'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "eZ1LGKKOzJKX",
        "outputId": "7096fcdb-4583-4aa9-80f3-14884e91cb66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        course_id  num_subscribers\n",
              "subject             level                                         \n",
              "Business Finance    All Levels                693          1047208\n",
              "                    Beginner Level            339           647007\n",
              "                    Expert Level               31            30146\n",
              "                    Intermediate Level        128           144350\n",
              "Graphic Design      All Levels                298           688332\n",
              "                    Beginner Level            242           315757\n",
              "                    Expert Level                5             1008\n",
              "                    Intermediate Level         57            58051\n",
              "Musical Instruments All Levels                276           328170\n",
              "                    Beginner Level            296           429030\n",
              "                    Expert Level                7             5431\n",
              "                    Intermediate Level        101            84058\n",
              "Web Development     All Levels                658          4808081\n",
              "                    Beginner Level            391          2660049\n",
              "                    Expert Level               15            13611\n",
              "                    Intermediate Level        135           455546"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cd8b7e7d-b547-40be-ab50-bdaea7a707ef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>course_id</th>\n",
              "      <th>num_subscribers</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>subject</th>\n",
              "      <th>level</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">Business Finance</th>\n",
              "      <th>All Levels</th>\n",
              "      <td>693</td>\n",
              "      <td>1047208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Beginner Level</th>\n",
              "      <td>339</td>\n",
              "      <td>647007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Expert Level</th>\n",
              "      <td>31</td>\n",
              "      <td>30146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Intermediate Level</th>\n",
              "      <td>128</td>\n",
              "      <td>144350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">Graphic Design</th>\n",
              "      <th>All Levels</th>\n",
              "      <td>298</td>\n",
              "      <td>688332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Beginner Level</th>\n",
              "      <td>242</td>\n",
              "      <td>315757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Expert Level</th>\n",
              "      <td>5</td>\n",
              "      <td>1008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Intermediate Level</th>\n",
              "      <td>57</td>\n",
              "      <td>58051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">Musical Instruments</th>\n",
              "      <th>All Levels</th>\n",
              "      <td>276</td>\n",
              "      <td>328170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Beginner Level</th>\n",
              "      <td>296</td>\n",
              "      <td>429030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Expert Level</th>\n",
              "      <td>7</td>\n",
              "      <td>5431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Intermediate Level</th>\n",
              "      <td>101</td>\n",
              "      <td>84058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">Web Development</th>\n",
              "      <th>All Levels</th>\n",
              "      <td>658</td>\n",
              "      <td>4808081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Beginner Level</th>\n",
              "      <td>391</td>\n",
              "      <td>2660049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Expert Level</th>\n",
              "      <td>15</td>\n",
              "      <td>13611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Intermediate Level</th>\n",
              "      <td>135</td>\n",
              "      <td>455546</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd8b7e7d-b547-40be-ab50-bdaea7a707ef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cd8b7e7d-b547-40be-ab50-bdaea7a707ef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cd8b7e7d-b547-40be-ab50-bdaea7a707ef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.\tWhat issues are typically associated with imbalanced data, and how do you handle it?"
      ],
      "metadata": {
        "id": "eQefhFFVzWbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalanced data means that the frequency of data points of one class is much higher (majority class) than of the other class (minority class).\n",
        "\n",
        "Building a classifier on imbalanced data means it will perform well on the majority class and perform poorly on the minority class.\n",
        "\n",
        "Typical issue in:\n",
        "- Disease diagnosis\n",
        "- Customer churn prediction\n",
        "- Fraud detection\n",
        "- Natural disaster\n",
        "\n",
        "Example: Cancer screening where 95/100 observations do not have cancer and 5/100 do. The model may classify that all 100 do not have cancer due to the overweight of this class. Accuracy is 95% but the error is critical.\n",
        "\n",
        "Methods to handle imbalanced data:\n",
        "- Resampling (over- and undersampling)\n",
        "- Synthetic Minority Oversampling Technique (SMOTE)\n",
        "- Balanced Bagging Classifier\n",
        "- Threshold Moving"
      ],
      "metadata": {
        "id": "RV4mr8yIgDB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resampling (over- and undersampling)"
      ],
      "metadata": {
        "id": "jPE4RavQj2CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oversampling: oversampling the minority class using replacement.\n",
        "\n",
        "Undersampling: undersampling the majority class by removing random rows\n",
        "\n",
        "Once both classes have a similar size, it is assumed the classifier will give equal importance to both classes."
      ],
      "metadata": {
        "id": "WhG5nulWpjm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/0*FeIp1t4uEcW5LmSM.png)"
      ],
      "metadata": {
        "id": "xmhGilyF4Dvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthetic Minority Oversampling Technique (SMOTE)"
      ],
      "metadata": {
        "id": "X1gqT3-XpnQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In SMOTE new instances are synthesized from the existing data. SMOTE looks into minority class instances and uses k nearest neighbor to select a random nearest neighbor, and a synthetic instance is created randomly in the feature space."
      ],
      "metadata": {
        "id": "moy2I1gFpsMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/1*XvKkuORUzmJaz042NgB0oA.png)"
      ],
      "metadata": {
        "id": "jdZ3w3rY55ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balanced Bagging Classifier"
      ],
      "metadata": {
        "id": "geY-v9N_r6TR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This classifier takes two special parameters:\n",
        "\n",
        "- Sampling_strategy: decides the type of resampling required, e.g.:\n",
        "-- majority: resample only the majority class\n",
        "-- all: resample all classes\n",
        "\n",
        "- Replacement: decides whether it is going to be a sample with replacement or not."
      ],
      "metadata": {
        "id": "VSPUmtTTsE-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/The-Balanced-Bagging-flow.ppm.png)"
      ],
      "metadata": {
        "id": "WRR26aiM6CgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Threshold Moving"
      ],
      "metadata": {
        "id": "FkdslTQTs6Ad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, class membership is often based on a 0.5 threshold (i.e. >0.5 belongs to one class and <0.5 belong to the other).\n",
        "\n",
        "For imbalanced data, the threshold may be changed to an optimum value to efficiently separate two classes. To find the threshold one can use:\n",
        "- ROC curve\n",
        "- Precision-Recall curves\n",
        "- Grid Search"
      ],
      "metadata": {
        "id": "SO3GHMDAtB4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.\tWhen should you use classification and when regression? SML"
      ],
      "metadata": {
        "id": "6g9yzWE73bG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use classification for discrete values and regression for continuous values"
      ],
      "metadata": {
        "id": "J8UFCOYH60qP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/NadiaHolmlund/M1_exam/main/Classification-vs-Regression.jpg)"
      ],
      "metadata": {
        "id": "uK8fRQxS3d8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification algorithms\n"
      ],
      "metadata": {
        "id": "CxPolCeF0rU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used to predict/classify discrete values such as Male or Female, True or False, Spam or Not Spam, etc.\n",
        "\n",
        "The task of the classification algorithm is to find the mapping function to map the input(x) to the discrete output(y).\n",
        "\n",
        "Types of ML Classification Algorithms:\n",
        "- Logistic Regression (Elastic net)\n",
        "- K-Nearest Neighbours\n",
        "- XGBclassifier\n",
        "- Support Vector Machines\n",
        "- Kernel SVM\n",
        "- Naïve Bayes\n",
        "- Decision Tree Classification\n",
        "- Random Forest Classification"
      ],
      "metadata": {
        "id": "udnQpERd7NGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression algorithms\n"
      ],
      "metadata": {
        "id": "M7pBL7Em6Y9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used to predict the continuous values such as price, salary, age, etc.\n",
        "\n",
        "The task of the regression algorithm is to find the mapping function to map the input variable(x) to the continuous output variable(y).\n",
        "\n",
        "Types of Regression Algorithms:\n",
        "- XGBregressor\n",
        "- Simple Linear Regression (OLS)\n",
        "- Multiple Linear Regression\n",
        "- Polynomial Regression\n",
        "- Support Vector Regression\n",
        "- Decision Tree Regression\n",
        "- Random Forest Regression"
      ],
      "metadata": {
        "id": "T1x6omEI7K9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.\tWhat is the difference between Supervised Learning and Unsupervised Learning?"
      ],
      "metadata": {
        "id": "RF8oZpoV8gNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised ML: labelled data\n"
      ],
      "metadata": {
        "id": "nTbY9I3sAAfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Classification: use an algorithm to accurately assign test data into specific categories (discrete values)\n",
        "- Regression: uses an algorithm to understand the relationship between dependent and independent variables (continuous values)"
      ],
      "metadata": {
        "id": "yU2iKZAgBmpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised ML: unlabelled data\n"
      ],
      "metadata": {
        "id": "ZXexvrkXACZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Clustering: grouping unlabelled data based on their similarities or differences\n",
        "- Association: uses different rules to find relationships between variables in a given dataset (e.g. recommender systems)\n",
        "- Dimensionality reduction: used when the number of features (or dimensions) in a given dataset is too high. Reduces the number of data inputs to a manageable size while also preserving the data integrity."
      ],
      "metadata": {
        "id": "Bqk0lLdHBpcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goals of ML:\n"
      ],
      "metadata": {
        "id": "KhRqh7EsAKoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Supervised: predict outcomes for new data\n",
        "- Unsupervised: get insights from large volumes of new data"
      ],
      "metadata": {
        "id": "uKrIPqE0BtRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/Screenshot%202022-10-07%20at%2021.48.06.png)"
      ],
      "metadata": {
        "id": "ircOzhDW_jtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.\tHow do you evaluate performance when training a regression model?"
      ],
      "metadata": {
        "id": "Wdv8gXGQA2O0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- R Square / Adjusted R Square\n",
        "- Mean Square Error (MSE) / Root Mean Squared Error (RMSE)\n",
        "- Mean Absolute Error(MAE)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/R-squared-formula-linear-regression-model-640x271.jpg)\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/1*5OQunI-NR-S0gAZFIit1Rw.png)"
      ],
      "metadata": {
        "id": "DdsKy2zHbtkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## R Square / Adjusted R Square"
      ],
      "metadata": {
        "id": "4Usp8QUFBeLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R sqaure measures how much variability in the dependant variable that can be explained by the model.\n",
        "\n",
        "R Square measures how much variability in dependent variable can be explained by the model. It is the square of the Correlation Coefficient(R) and that is why it is called R Square.\n",
        "\n",
        "R Square is calculated by the sum of squared of prediction error divided by the total sum of the square which replaces the calculated prediction with mean. R Square value is between 0 to 1 and a bigger value indicates a better fit between prediction and actual value.\n",
        "\n",
        "Adjusted R Square will penalize additional independent variables added to the model and adjust the metric to prevent overfitting issues. \n",
        "\n",
        "From the sample model, we can interpret that around 79% of dependent variability can be explained by the model, and adjusted R Square is roughly the same as R Square meaning the model is quite robust."
      ],
      "metadata": {
        "id": "JpuZ9aEsB1ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean Square Error (MSE) / Root Mean Squared Error (RMSE)"
      ],
      "metadata": {
        "id": "UN7V_aS2BgKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While R Square is a relative measure of how well the model fits dependent variables, Mean Square Error is an absolute measure of the goodness for the fit.\n",
        "MSE is calculated by the sum of square of prediction error which is real output minus predicted output and then divide by the number of data points. It gives you an absolute number on how much your predicted results deviate from the actual number. You cannot interpret many insights from one single result but it gives you a real number to compare against other model results and help you select the best regression model.\n",
        "Root Mean Square Error(RMSE) is the square root of MSE. It is used more commonly than MSE because firstly sometimes MSE value can be too big to compare easily. Secondly, MSE is calculated by the square of error, and thus square root brings it back to the same level of prediction error and makes it easier for interpretation."
      ],
      "metadata": {
        "id": "Qn_16_BsCGD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean Absolute Error(MAE)"
      ],
      "metadata": {
        "id": "8xrQz5LRBhTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Absolute Error(MAE) is similar to Mean Square Error(MSE). However, instead of the sum of square of error in MSE, MAE is taking the sum of the absolute value of error.\n",
        "Compare to MSE or RMSE, MAE is a more direct representation of sum of error terms. MSE gives larger penalization to big prediction error by square it while MAE treats all errors the same."
      ],
      "metadata": {
        "id": "JZRJec97CK71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8(2) How do you evaluate performance when training a classification model?"
      ],
      "metadata": {
        "id": "zEsBRcqNApG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Precision - Positive predictive value (PPV)\n",
        "- Recall - True positive rate (TPR)\n",
        "- Accuracy - ACC\n",
        "- Specificity - True negative rate (TNR)\n",
        "- F1 Score - Weighted avg. of TPR and PPV\n",
        "- Confusion Matrix - 2x2 with TP, TN, FP, FN\n",
        "- AUC ROC curve - (Area Under The Curve) + (Receiver Operating Characteristics)"
      ],
      "metadata": {
        "id": "pUEqb7dGAmtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/PPV.png) Precision - Positive predictive value (PPV) \n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/TPR.png) Recall - True positive rate (TPR) \n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/ACC.png) Accuracy - ACC \n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/TNR.png) Specificity - True negative rate (TNR)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/F1.png) F1 Score - Weighted avg. of TPR and PPV \n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/confusion%20matrix.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/ROC.png)"
      ],
      "metadata": {
        "id": "L6vQinmNMy4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How do you evaluate performance in lecture notebooks"
      ],
      "metadata": {
        "id": "vBhkjGamiAyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "score (determine which score type to use, e.g. accuracy, MSE etc.)\n",
        "\n",
        "Confusion matrix\n",
        "\n",
        "Classification report\n",
        "- Precision\n",
        "- Recall\n",
        "- F1 score"
      ],
      "metadata": {
        "id": "4ZtxKP0fis0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.\tHow can you treat missing values?"
      ],
      "metadata": {
        "id": "CO-YnAXWB-3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Do We Need To Care About Handling Missing Data?"
      ],
      "metadata": {
        "id": "Y0g4adwkcPjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most ML algorithms fail on missing values (except K-nearest and Naive Bayes)\n",
        "- Incorrect handling of missing values may lead to bias, i.e. incorrect results\n",
        "- Missing values can lead to lack of precision"
      ],
      "metadata": {
        "id": "stUCedg3cNn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of missing values"
      ],
      "metadata": {
        "id": "QqHT07Q4bMM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Missing Completely At Random (MCAR): Missing values are completely independent of other data. No pattern.\n",
        "- Missing At Random (MAR): The probability of data missing depends only on the observed data. Pattern.\n",
        "- Missing Not At Random (MNAR): There is some pattern in missing data but other observed data can not explain it. Pattern but it's unknown."
      ],
      "metadata": {
        "id": "c00tfKJtbVNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How To Handle Missing Values?"
      ],
      "metadata": {
        "id": "nR3iJbTUdc2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deleting missing values\n",
        "- Deleting the entire row\n",
        "- Deleting the entire column\n",
        "\n",
        "Pros: a model trained with the removal of all missing values creates a robust model\n",
        "\n",
        "Cons: Loss of informations Works poortly is the % of missing values is excessive."
      ],
      "metadata": {
        "id": "YMFqleBhnS01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputing missing values\n",
        "- Replacing With Arbitrary Value - make an educated guess about the missing value and replace it with an arbitrary (random) value \n",
        "- Replacing With Mean - average\n",
        "- Replacing With Mode - most frequent occuring value\n",
        "- Replacing With Median - middle value in a sortet dataset\n",
        "- Replacing with Previous Value – Forward Fill\n",
        "- Replacing with Next Value – Backward Fill\n",
        "- Interpolation - generating points between given points (‘polynomial’, ‘linear’, ‘quadratic’)\n",
        "\n",
        "Pros: prevent data loss which results in deletion of rows or columns & works well w/ small data set and easy to implement\n",
        "\n",
        "Cons: Works only with numerical continous variables. Can casue data leakage & does not factor the covariance between features"
      ],
      "metadata": {
        "id": "HygmfYAKnX_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputing missing values for categorical features\n",
        "- Impute the Most Frequent Value\n",
        "- Impute the Value “missing”, which treats it as a Separate Category\n",
        "\n",
        "Pros: prevent data loss which results in deletion of rows or columns. Works well w/small dataset and easy to implement. Negates the loss of data by adding a unique category\n",
        "\n",
        "Cons: Works only w/categorical variables Addition of new features to the model while encoding may result in poor performance"
      ],
      "metadata": {
        "id": "78I7gnWKbJhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputation using sci-kit learn library"
      ],
      "metadata": {
        "id": "t2qxz9lsgDk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Univariate Approach:\n",
        "- only a single feature is taken into consideration. You can use the class SimpleImputer and replace the missing values with mean, mode, median or some constant value.\n",
        "\n",
        "Multivariate Approach:\n",
        "- more than one feature is taken into consideration. There are two ways to impute missing values considering the multivariate approach. Using KNNImputer or IterativeImputer classes\n",
        "\n",
        "Nearest Neighbors Imputations (KNNImputer):\n",
        "- Missing values are imputed using the k-Nearest Neighbors approach where a Euclidean distance is used to find the nearest neighbors.\n",
        "\n",
        "Multivariate imputations (IterativeImputer):\n",
        "- A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion.\n",
        "\n",
        "Adding missing indicator to encode “missingness” as a feature:\n",
        "- In some cases, while imputing missing values, you can preserve information about which values were missing and use that as a feature.\n",
        "Because sometimes there may be a relationship between the reason for missing values (also called the “missingness”) and the target variable you are trying to predict."
      ],
      "metadata": {
        "id": "k0GMbDr5fXqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.\tWhat do you understand by Precision and Recall?"
      ],
      "metadata": {
        "id": "OhwLaj4zX1J6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision:\n",
        "- Expresses the proportion of data points the model says were relevant that actually were relevant.\n",
        "- How many retrieved elements are relevant?\n",
        "\n",
        "Recall:\n",
        "-  Expresses the proportion of actual positives the model classified correctly.\n",
        "- How many relevant items are retrieved?\n",
        "\n",
        "Often inverse relationship with each other."
      ],
      "metadata": {
        "id": "Wjjyjb8yggGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/Screenshot%202022-10-08%20at%2000.27.54.png)"
      ],
      "metadata": {
        "id": "ca6OnrrukHvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.\tCan you cite some examples where false positives are more problematic than false negatives?"
      ],
      "metadata": {
        "id": "vSVAPQZlmcm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples:\n",
        "1. A prenatal test comes back positive for Down’s Syndrome or a disease, when the fetus does not have the disorder. Might cause some people to terminate the pregnancy\n",
        "2. False positive Covid tests, which forced people to isolate for 10 days\n",
        "2. When credit cards gets blocked for theft or unusual behaviour, when it is in fact not. May prevent people from payments due\n"
      ],
      "metadata": {
        "id": "fQb6VElYYKO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.\tWhat are Recommender Systems?"
      ],
      "metadata": {
        "id": "RSknFN-pZmwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A recommender system seeks to predict the rating or the preference a user might give to an item, i.e. it is an algorithm that suggests relevant items to user. (products, content, advertisements, information etc).\n",
        "\n",
        "Methods:\n",
        "- Collaborative filtering\n",
        "- Content based filtering"
      ],
      "metadata": {
        "id": "2IlkqhnzmlMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/Content-based-filtering-vs-Collaborative-filtering-Source.png)"
      ],
      "metadata": {
        "id": "EWLpKlQ_zRuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13.\tWhat is Collaborative filtering?"
      ],
      "metadata": {
        "id": "7ZrvUIeXZ0F4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collaborative filtering:\n",
        "- Based solely on  past interactions recorded between users and items in order to produce new recommendations.\n",
        "-- Explicit interaction: e.g. rating on a movie\n",
        "-- Implicit interaction: e.g. watching a movie\n",
        "- Uses similarities between users and items simultaneously to provide recommendations\n",
        "- Interactions are stored in the so-called “user-item interactions matrix”.\n",
        "\n",
        "Classical collaborative filtering approaches:\n",
        "- Memory based methods\n",
        "-- user-user\n",
        "-- item-item\n",
        "- Model based approach\n",
        "-- matrix factorisation\n",
        "\n",
        "Cold start problem:\n",
        "- Inefficient to recommend to new users\n",
        "- Inefficient to recommend a new item to any users\n",
        "- Inefficient to recommend items with few interactions\n",
        "\n",
        "Solutions:\n",
        "- Random strategy: recommending random items to new users or new items to random users\n",
        "- Maximum expectation strategy: recommending popular items to new users or new items to most active users\n",
        "- exploratory strategy: recommending a set of various items to new users or a new item to a set of various users\n",
        "- Using a non collaborative method for the early life of the user or the item."
      ],
      "metadata": {
        "id": "fY3o8zkCwq9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13(2) What is Content based filtering?"
      ],
      "metadata": {
        "id": "XYqn3MvvtiWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Content based filtering:\n",
        "- rely on user-item interactions + additional information about users and/or items. e.g. gender, job, genres, authors, actors, duration etc.\n",
        "- Build a model based on available 'features', that explain the observed user-item interactions.\n",
        "\n",
        "Classical collaborative filtering approaches:\n",
        "- Various classification models\n",
        "- Various regression models\n",
        "\n",
        "Cold start problem:\n",
        "- Suffers less than collaborative, new users or items can be described by their characteristics (content) and so relevant suggestions can be done for these new entities\n",
        "- Only new users or items with previously unseen features will suffer, but with older/bigger systems this is unlikely to happen."
      ],
      "metadata": {
        "id": "sKKIFzV2tyiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14.\tHow can outlier values be treated?"
      ],
      "metadata": {
        "id": "bftSxYhe0Eq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few outliers can ruin an ML algorithms performance, hence the importance of detection and correct handling."
      ],
      "metadata": {
        "id": "CJRHTW_h0Nz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detecting outliers"
      ],
      "metadata": {
        "id": "PlcDLfHB0G9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methods:\n",
        "- Boxplot\n",
        "- Histogram\n",
        "- Mean and Standard Deviation\n",
        "- IQR (Inter Quantile Range):\n",
        "-- Criteria: data points that lie 1.5 times of IQR above Q3 and below Q1 are outliers.\n",
        "- Z-score Percentile:\n",
        "-- Criteria: any data point whose Z-score ((Xi-mean)/std) falls out of 3rd standard deviation is an outlier.\n"
      ],
      "metadata": {
        "id": "Gz8-VP5v0KDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treating outliers"
      ],
      "metadata": {
        "id": "FB1GOmab075X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trimming / removing the outlier: remove the outliers from the dataset\n",
        "- Quantile based flooring and capping: the outlier is capped at a certain value above the 90th percentile value or floored at a factor below the 10th percentile value. The data points that are lesser than the 10th percentile are replaced with the 10th percentile value and the data points that are greater than the 90th percentile are replaced with 90th percentile value\n",
        "- Mean/Median imputation: As the mean value is highly influenced by the outliers, it is advised to replace the outliers with the median value.\n",
        "- Binning the data and categorizing them will totally avoid the outliers. It will make the data categorical instead.\n",
        "- Consider them as null values and fill them up using techniques of filling up null values"
      ],
      "metadata": {
        "id": "wL0mPULf06nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/IQR-1.png)\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/440971.png)"
      ],
      "metadata": {
        "id": "jWnMrjRu3Q-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15.\tWhat do we use dimensionality reduction for?"
      ],
      "metadata": {
        "id": "B0rRTFq8cfTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In ML, dimensionality simply refers to the number of features/variables in the dataset. Dimensionality reduction is the task of reducing the number of features/variables without losing information and explainability.\n",
        "\n",
        "Curse of dimenstionality:\n",
        "When the number of features is high compared to number of observations\n",
        "- Makes it hard for certain algorithms to train effective models\n"
      ],
      "metadata": {
        "id": "6wt4BVyCjEnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why is DR important"
      ],
      "metadata": {
        "id": "mhGRfNdskWVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- HD often lead to over-fitting (perform well on training, poorly on test)\n",
        "- HD can hamper the accuracy of algorithms because it includes irrelevant features (HD features are often correlated, hence redundant)\n",
        "- DR reduces the number features, while preserving most of the relevant information\n",
        "- DR makes it easier to visualize, especially if reduced to 2D or 3D\n",
        "- DR avoids the curse of dimensionality\n",
        "- DR reduces the time and storage space required\n",
        "\n"
      ],
      "metadata": {
        "id": "6CarbqVzkYF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Most used DR techniques:"
      ],
      "metadata": {
        "id": "99SUs6T1jqmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PCA\n",
        "- Truncated SVD\n",
        "- NMF\n",
        "- UMAP\n",
        "- t-SNE\n",
        "- (M)CA\n",
        "- LDA\n",
        "\n",
        "Principal Component Analysis (PCA):\n",
        "- Used to summarize information contained in continuous (i.e, quantitative) multivariate data by reducing the dimensionality of the data without losing important information.\n",
        "\n",
        "Truncated SVD\n",
        "- PCA is (truncated) SVD on centered data (by per-feature mean substraction). If the data is already centered, those two classes will do the same.\n",
        "- In practice TruncatedSVD is useful on large sparse datasets which cannot be centered without making the memory usage explode.\n",
        "\n",
        "Non-negative matrix factorization (NMF or NNMF):\n",
        "- Similar to PCA but only to be used on positive values and Unlike PCA, the representation of a vector is obtained in an additive fashion. Identifies human-interpretable “patterns” or “topics”.\n",
        "\n",
        "Uniform Manifold Approximation and Projection (UMAP):\n",
        "- Recent scalable approach for visualization and general non-linear dimension reduction.\n",
        "\n",
        "t-SNE (replaced by UMAP)\n",
        "\n",
        "(Multiple) Correspondence Analysis ((M)CA):\n",
        "- An extension of the principal component analysis suited to analyse a large contingency table formed by two qualitative variables (or categorical data).\n",
        "\n",
        "Latent Dirichlet Allocation (LDA):\n",
        "- Generative probabilistic model for collections of discrete dataset such as text corpora, where abstract topics need to be found."
      ],
      "metadata": {
        "id": "hokGGdi04jti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16.\tExplain clustering and some examples of typical algorithms."
      ],
      "metadata": {
        "id": "AcZHMzpCe-kY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering groups objects in a way where objects in the same cluster are more similar to each other than to objects in other clusters.\n",
        "\n",
        "Subgroups of clustering:\n",
        "- Hard clustering: 1 observation belongs to 1 cluster (possibility of not falling into a cluster at all)\n",
        "- Soft clustering: 1 observations can belong to several clusters with some probability."
      ],
      "metadata": {
        "id": "JviTFV53n7FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cluster models (how the algorithm form clusters)"
      ],
      "metadata": {
        "id": "oymPARJDouhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connectivity-based clustering:\n",
        "- Data points that are closer in the data space are more related (similar) than to data points farther away.\n",
        "\n",
        "Centroid-based clustering:\n",
        "- Clusters are represented by a central vector or a centroid. This centroid might not necessarily be a member of the dataset. (e.g. k-means)\n",
        "\n",
        "Distribution-based clustering:\n",
        "- Groups data based on their likely hood of belonging to the same probability distribution (Gaussian, Binomial, etc.)\n",
        "\n",
        "Density-based methods:\n",
        "- Groups data based on the idea that a cluster in a data space is a contiguous region of high point density, separated from other such clusters by contiguous regions of low point density."
      ],
      "metadata": {
        "id": "9TBIJl9v6T4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering algorithms"
      ],
      "metadata": {
        "id": "4nPDcbx6qCIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means:\n",
        "- K-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster. Uses the Euclidean distances between the points as a criterion for cluster formation\n",
        "\n",
        "Mini-Batch K-Means:\n",
        "- Mini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n",
        "\n",
        "Affinity Propagation:\n",
        "- Affinity Propagation involves finding a set of exemplars that best summarize the data.\n",
        "\n",
        "Agglomerative Clustering:\n",
        "- Agglomerative clustering involves merging examples until the desired number of clusters is achieved.\n",
        "\n",
        "BIRCH:\n",
        "- BIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using\n",
        "Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n",
        "\n",
        "DBSCAN:\n",
        "- DBSCAN Clustering (where DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise) involves finding high-density areas in the domain and expanding those areas of the feature space around them as clusters.\n",
        "\n",
        "Mean Shift:\n",
        "- Mean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.\n",
        "\n",
        "OPTICS:\n",
        "- OPTICS clustering (where OPTICS is short for Ordering Points To Identify the Clustering Structure) is a modified version of DBSCAN described above.\n",
        "\n",
        "Spectral Clustering:\n",
        "- Spectral Clustering is a general class of clustering methods, drawn from linear algebra.\n",
        "\n",
        "Mixture of Gaussians:\n",
        "- A Gaussian mixture model summarizes a multivariate probability density function with a mixture of Gaussian probability distributions as its name suggests.\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/1*oNt9G9UpVhtyFLDBwEMf8Q.png)"
      ],
      "metadata": {
        "id": "296WqHkyqBKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17.\tHow do you select important variables while working on a data set?"
      ],
      "metadata": {
        "id": "tN7ti-qO6ay3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select variables by feature selection or feature extraction"
      ],
      "metadata": {
        "id": "YR7GgmcP6bmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter methods"
      ],
      "metadata": {
        "id": "fu70Aq120Vl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter methods pick up the intrinsic properties of the features measured via univariate statistics instead of cross-validation performance. These methods are faster and less computationally expensive than wrapper methods.\n",
        "\n",
        "Information Gain:\n",
        "- Information gain calculates the reduction in entropy from the transformation of a dataset. It can be used for feature selection by evaluating the Information gain of each variable in the context of the target variable.\n",
        "\n",
        "Chi-square Test:\n",
        "- The Chi-square test is used for categorical features in a dataset. We calculate Chi-square between each feature and the target and select the desired number of features with the best Chi-square scores. In order to correctly apply the chi-squared in order to test the relation between various features in the dataset and the target variable, the following conditions have to be met: the variables have to be categorical, sampled independently and values should have an expected frequency greater than 5.\n",
        "\n",
        "Fisher’s Score:\n",
        "- Fisher score is one of the most widely used supervised feature selection methods. The algorithm which we will use returns the ranks of the variables based on the fisher’s score in descending order. We can then select the variables as per the case.\n",
        "\n",
        "Correlation Coefficient:\n",
        "- Correlation is a measure of the linear relationship of 2 or more variables. Through correlation, we can predict one variable from the other. The logic behind using correlation for feature selection is that the good variables are highly correlated with the target. Furthermore, variables should be correlated with the target but should be uncorrelated among themselves.\n",
        "- If two variables are correlated, we can predict one from the other. Therefore, if two features are correlated, the model only really needs one of them, as the second one does not add additional information. We will use the Pearson Correlation here.\n",
        "\n",
        "Variance Threshold:\n",
        "- The variance threshold is a simple baseline approach to feature selection. It removes all features which variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples. We assume that features with a higher variance may contain more useful information, but note that we are not taking the relationship between feature variables or feature and target variables into account, which is one of the drawbacks of filter methods.\n",
        "\n",
        "Mean Absolute Difference (MAD):\n",
        "- ‘The mean absolute difference (MAD) computes the absolute difference from the mean value. The main difference between the variance and MAD measures is the absence of the square in the latter. The MAD, like the variance, is also a scale variant.’ This means that higher the MAD, higher the discriminatory power.\n",
        "\n",
        "Dispersion ratio:\n",
        "- ‘Another measure of dispersion applies the arithmetic mean (AM) and the geometric mean (GM). For a given (positive) feature Xi on n patterns. Higher dispersion implies a higher value of Ri, thus a more relevant feature. Conversely, when all the feature samples have (roughly) the same value, Ri is close to 1, indicating a low relevance feature.’"
      ],
      "metadata": {
        "id": "g4jxkY262Mp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapper methods"
      ],
      "metadata": {
        "id": "Zay2ROEG0XLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrappers require some method to search the space of all possible subsets of features, assessing their quality by learning and evaluating a classifier with that feature subset. The feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion. The wrapper methods usually result in better predictive accuracy than filter methods.\n",
        "\n",
        "Forward Feature Selection:\n",
        "- This is an iterative method wherein we start with the best performing variable against the target. Next, we select another variable that gives the best performance in combination with the first selected variable. This process continues until the preset criterion is achieved.\n",
        "\n",
        "Backward Feature Elimination:\n",
        "- This method works exactly opposite to the Forward Feature Selection method. Here, we start with all the features available and build a model. Next, we the variable from the model which gives the best evaluation measure value. This process is continued until the preset criterion is achieved.\n",
        "\n",
        "Exhaustive Feature Selection:\n",
        "- This is the most robust feature selection method covered so far. This is a brute-force evaluation of each feature subset. This means that it tries every possible combination of the variables and returns the best performing subset.\n",
        "\n",
        "Recursive Feature Elimination:\n",
        "- ‘Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute.\n",
        "- Then, the least important features are pruned from the current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.’[2]"
      ],
      "metadata": {
        "id": "NdghN0lK2ItY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedded methods"
      ],
      "metadata": {
        "id": "-HbAAy1z0ZAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These methods encompass the benefits of both the wrapper and filter methods, by including interactions of features but also maintaining reasonable computational cost. Embedded methods are iterative in the sense that takes care of each iteration of the model training process and carefully extracts those features which contribute the most to the training for a particular iteration.\n",
        "\n",
        "LASSO Regularization (L1):\n",
        "- Regularization consists of adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model, i.e. to avoid over-fitting. In linear model regularization, the penalty is applied over the coefficients that multiply each of the predictors. From the different types of regularization, Lasso or L1 has the property that is able to shrink some of the coefficients to zero. Therefore, that feature can be removed from the model.\n",
        "\n",
        "Random Forest Importance:\n",
        "- Random Forests is a kind of a Bagging Algorithm that aggregates a specified number of decision trees. The tree-based strategies used by random forests naturally rank by how well they improve the purity of the node, or in other words a decrease in the impurity (Gini impurity) over all trees. Nodes with the greatest decrease in impurity happen at the start of the trees, while notes with the least decrease in impurity occur at the end of trees. Thus, by pruning trees below a particular node, we can create a subset of the most important features."
      ],
      "metadata": {
        "id": "_HERsun22rou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19.\tWhat is scaling? Where is it used? Which types do exist?"
      ],
      "metadata": {
        "id": "wk6LAIzS6b8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is scaling and where is it used"
      ],
      "metadata": {
        "id": "9fmEAiO13OwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.\n",
        "\n",
        "e.g. an algorithm with consider 3000 meters to be greater than 5 km, which results in wrong predictions. Feature scaling brings all values to the same magnitude, tackling the issue."
      ],
      "metadata": {
        "id": "FkHuvhmL3NVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Techniques for scaling"
      ],
      "metadata": {
        "id": "SiYHqiCH3SKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization (sklearn: MinMaxScaler):\n",
        "- This technique re-scales a feature or observation value with distribution value between 0 and 1.\n",
        "- Only required when the data distribution is unknown or the data doesn't have Gaussian Distribution. This type of scaling technique is used when the data has a diversified scope and the algorithms on which the data are being trained do not make presumptions about the data distribution\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/1*mHmxR4CMM7aQFcrX1WvpNw.jpeg)\n",
        "\n",
        "Standardization (sklearn: StandardScaler):\n",
        "- It is a very effective technique which re-scales a feature value so that it has distribution with 0 mean value and variance equals to 1.\n",
        "- Usually preferred when the data is being used for multivariate analysis i.e. when we want all the variables of comparable units. It is usually applied when the data has a bell curve i.e. it has gaussian distribution. No this isn't always true but is considered more effective when applied to Gaussian distribution. This technique comes in handy when the data has varying ratios and the algorithms used, make assumptions about the data distribution\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/1*-V6TGYz2E-kkzgxGuwmnpg.jpeg)\n",
        "\n",
        "Robust Scalar:\n",
        "- Robust scaling is one of the best scaling techniques when we have outliers present in our dataset. It scales the data accordingly to the interquartile range (IQR = 75 Quartile — 25 Quartile).\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/1*mwdkncyXehKKkmWBs9-ECg.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/1*HW7-kYjj6RKwrO-5WTLkDA.png)\n"
      ],
      "metadata": {
        "id": "E-J7E9sC3s2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20\tHow would you visualize multidimensional data"
      ],
      "metadata": {
        "id": "kfBhaNjT8X02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Dimension\n",
        "- histograms\n",
        "- density plot\n",
        "- bar plots\n",
        "- pie-charts"
      ],
      "metadata": {
        "id": "XUI0O9BR8hxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Dimensions\n",
        "- pair-wise correlation matrix as heatmap\n",
        "- pair-wise scatter plots\n",
        "- parallel coordinates\n",
        "- joint plots\n",
        "- stacked bars or multiple bars\n",
        "- box plots\n",
        "- violin plots\n",
        "- faceting\\subplots along with generic histograms or density plots.\n",
        "\n",
        "\n",
        "To accomodate more than 2 dimensions introduce:\n",
        "- 3 axis (length, breadth, depth)\n",
        "- Kernel density plot\n",
        "\n",
        "Maxing out dimensions in seaborn:\n",
        "- x=\"flipper_length_mm\", \n",
        "- y=\"bill_length_mm\",\n",
        "- kind=\"scatter\",\n",
        "- hue='species',\n",
        "- style= 'sex',\n",
        "- size = \"body_mass_g\",\n",
        "- col = 'island')"
      ],
      "metadata": {
        "id": "ZYsEXgBm8jOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://seaborn.pydata.org/_images/function_overview_8_0.png)"
      ],
      "metadata": {
        "id": "YJuOiYxM_iid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21.\tWhat is over/under fitting, and how can you avoid It?\n"
      ],
      "metadata": {
        "id": "diZ3VNMmhp-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Underfitting:\n",
        "- High training error\n",
        "- Training error close to test error\n",
        "- High bias\n",
        "-- Bias Error: The simplifying assumptions made by a model to make the target function easier to learn. Generally, simple parametric algorithms have a high bias making them fast to learn and easier to understand but generally less flexible.\n",
        "\n",
        "Overfitting:\n",
        "- Very low training error\n",
        "- Training error much lower than test error\n",
        "- High variance\n",
        "-- Variance Error: Variance is the amount that the estimate of the target function will change if different data was used. Ideally, it should not change too much from one training dataset to the next, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y4WlyC8FERY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/1*9DtauyXaQFAOi31Pp1XQUg.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/72e222c1542539754df1d914cb671bd7.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/Bias-and-Variance-Model.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/Screenshot%202022-10-06%20at%2016.51.52.png)"
      ],
      "metadata": {
        "id": "JVn02_vsF2af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 22.\tWhat are ‘training Set’ and ‘test Set’ in a Machine Learning Model? How Much Data Will You Allocate for Your Training, Validation, and Test Sets?\n"
      ],
      "metadata": {
        "id": "6hpVUSjwovOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most supervised machine learning tasks, best practice recommends to split your data into three independent sets:\n",
        "- Training Set: For training of the model. \n",
        "- Validation Set: For unbiased evaluation of the model.\n",
        "- Test Set: For final evaluation of the model. \n",
        "\n",
        "Training Set:\n",
        "- The dataset that we feed our model to learn potential underlying patterns and relationships.\n",
        "- The training set should be as representative as possible of the population that we are trying to model. Additionally, we need to be careful and ensure that it is as unbiased as possible, as any bias at this stage may be propagated downstream during inference.\n",
        "\n",
        "Model building:\n",
        "- Building different models to classify/predict based on different features\n",
        "- We could compare the accuracy of each model on the training set, but if we use the same exact dataset for both training and tuning, the model will overfit and won't generalize well.\n",
        "\n",
        "Validation Set:\n",
        "- The dataset that we use to understand our model's performance across different model types and hyperparameter choices.\n",
        "- it acts as an independent, unbiased dataset for comparing the performance of different algorithms trained on our training set. \n",
        "- Used to identify the best model\n",
        "\n",
        "Test Set:\n",
        "- The dataset that we use to approximate our model's unbiased accuracy in the wild.\n",
        "- Once we have used the validation set to determine the algorithm and parameter choices that we would like to use in production, the test set is used to approximate the models's true performance in the wild. It is the final step in evaluating our model's performance on unseen data. \n",
        "- We should never, under any circumstance, look at the test set's performance before selecting a model. \n",
        "\n",
        "Peeking at our test set performance ahead of time is a form of overfitting, and will likely lead to unreliable performance expectations in production. It should only be checked as the final form of evaluation, after the validation set has been used to identify the best model.\n",
        "\n",
        "\n",
        "https://mlu-explain.github.io/train-test-validation/"
      ],
      "metadata": {
        "id": "E5C4nEyK6tJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional stuff"
      ],
      "metadata": {
        "id": "B0DwMB-PvuJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pickle\n",
        "\n",
        "Allows you to save a trained model to share, commit, and re-load it faster.\n",
        "\n",
        "1. Load data\n",
        "2. Train model -  split into test/train, instantiate model and fit\n",
        "3. Save the model pickle.dump(model, open('model.pkl', 'wb'))\n",
        "4. Load the model pickled_model = pickle.load(open('model.pkl', 'rb'))\n",
        "5. Use the pre-trained model to predict new data pickled_model.predict(X_test)\n",
        "\n",
        "Alternative is to save as json files\n",
        "\n",
        "Machine learning models often take hours or days to run, especially on large datasets with many features. If your machine goes off, you’ll lose your model and you’ll need to re-train it from scratch.\n",
        "\n"
      ],
      "metadata": {
        "id": "C9WDBtzh7HV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature importance / model explainability\n",
        "\n",
        "yellowbrick FeatureImportances\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/Screenshot%202022-10-09%20at%2014.58.21.png)\n",
        "\n",
        "ELI5\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/Screenshot%202022-10-09%20at%2014.58.48.png)\n",
        "\n",
        "LIME\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/Screenshot%202022-10-09%20at%2015.01.00.png)\n",
        "\n",
        "SHAP\n",
        "\n",
        "shap.summary_plot\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/Screenshot%202022-10-09%20at%2015.02.00.png)\n",
        "\n",
        "shap.summary_plot (type=bar)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/Screenshot%202022-10-09%20at%2015.42.35.png)\n",
        "\n",
        "shap.dependence_plot\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/Screenshot%202022-10-09%20at%2015.42.56.png)\n",
        "\n",
        "shap.initsj\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/Screenshot%202022-10-09%20at%2015.02.16.png)\n",
        "\n",
        "Elbow method:\n",
        "\n",
        "- x = number of clusters / features in PCA\n",
        "- Cumulative variance explainability / percent of variance explaned\n",
        "-- K-means score / sum of squared distances\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/1*8wV1j-klQA1xFvfaNXuVzg.png)"
      ],
      "metadata": {
        "id": "NjQaoaaBz6s2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SML in lecture notebooks\n",
        "\n",
        "Regression problem:\n",
        "1. Import data\n",
        "2. Specify target (y)\n",
        "3. Check out the correlation matrix and pairplot\n",
        "4. Preprocess data - Standardscaling (instantiate, fit, transform)\n",
        "5. Preprocess data - split into test and training set\n",
        "6. Instantiate the models\n",
        "7. Fit the models to training data (different models with increasing complexity and hyperparameter tuning options\n",
        "- OLS model (baseline)\n",
        "- Elastic Net (still parametric, but maybe advantages in feature selection)\n",
        "- Random forest regressor (tree-based ensemble model)\n",
        "8. Score the models on the test data using .score\n",
        "9. Tune hyperparameters using grid search and scorer as the scoring method. Fit grid to X and y and return the best estimator\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "When using scaling models:\n",
        "1. Instantiate the model\n",
        "2. Fit the data onto the model (The fit function computes the formulation to transform the column based on Standard scaling but doesn’t apply the actual transformation. The computation is stored as a fit object. The fit method doesn’t return anything.)\n",
        "3. Transform the data onto the dataset (The transform method takes advantage of the fit object in the fit() method and applies the actual transformation onto the column. So, fit() and transform() is a two-step process that completes the transformation in the second step. Here, Unlike the fit() method the transform method returns the actually transformed array.)"
      ],
      "metadata": {
        "id": "y_94pXPxOBL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/metrics.png)"
      ],
      "metadata": {
        "id": "GjXox0QupuLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Four types of data\n",
        "Nominal\n",
        "- categorical, equally good (cities, colours, gender)\n",
        "\n",
        "Ordinal\n",
        "- categorical, not equally good ([high, medium, low], [graduate, masters, phd])\n",
        "\n",
        "Discrete\n",
        "- countable integer values (number of students in a class, days in a week)\n",
        "\n",
        "Continous\n",
        "- Fractinoal float values (height, speed, temperature)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/types-of-data-.png)"
      ],
      "metadata": {
        "id": "kjhOtVF1FLTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overall ML process\n",
        "\n",
        "1.\tLoading data and pre-processing dataset of interest\n",
        "2.\tHyperparameter optimization using cross-validation\n",
        "3.\tFitting tuned algorithm to the training data\n",
        "4.\tApplying learned model to test data\n",
        "\n",
        "## Cross-validation\n",
        "Cross-validation is a resampling method of evaluating the validity of an ML model using a data sample. A technique that lets one to weigh the overfitting or underfitting extent of a model using the training data and testing data, cross-validation also allows one to test the accuracy of a model before launching it for public use. \n",
        " \n",
        "While there are numerous ways to cross-validate a model, all methods aim to test a data sample kept aside from the training data set.\n",
        "1.\tSeparate a part of the data set from the rest.\n",
        "2.\tUtilize the rest of the data set to train the ML model.\n",
        "3.\tOnce the model is ready, validate the model using the data set that was separated earlier.\n",
        " \n",
        "Types of cross-validation:\n",
        "1.\tHoldout Cross-Validation\n",
        "2.\tK-fold Cross-Validation\n",
        "3.\tStratified K-fold Cross-Validation\n",
        "4.\tLeave-p-out Cross-Validation\n",
        "5.\tLeave-one-out Cross-Validation\n",
        "6.\tRolling Cross-Validation\n",
        "7.\tMonte Carlo Cross-Validation\n",
        "\n",
        "## Grid Search / hyperparameter tuning / Regularization\n",
        "Grid-search is used to find the optimal hyperparameters of a model which results in the most ‘accurate’ predictions. Grid search can be used to improve any specific evaluation metric.\n",
        "\n",
        "The process of minimizing bias and variance by selecting\n",
        "•\tModel architecture\n",
        "•\tLearning rate\n",
        "•\tNumber of epochs\n",
        "•\tNumber of branches in a decision tree\n",
        "•\tNumber of clusters in a clustering algorithm\n",
        "•\tThe right model class\n",
        "•\tFunctional form\n",
        "•\tDegree of complexity\n",
        "\n",
        "## K-fold crossvalidation\n",
        "Advanced version of Out-of-Sample-Testing, where the process is repeated several times during the hyperparameter-tuning phase\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/Picture%201.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "JKCQZcqJHJ6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distances\n",
        "Cosine - used in collaborative recommendation systems\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/1*FTVRr_Wqz-3_k6Mk6G4kew.png)\n",
        "\n",
        "![](https://raw.github.com/NadiaHolmlund/M1_exam/main/cosine-similarity-vectors.original.jpg)"
      ],
      "metadata": {
        "id": "q03BdKzvt_kv"
      }
    }
  ]
}